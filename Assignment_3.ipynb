{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orchidObsessed/sbx_osu_cs4783/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Och5q0fSqVSd"
      },
      "source": [
        "# **Imports & Constants**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "y67dmY09qVSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7efb08-7421-4e7b-cc8a-46ebfbd45051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Mounting to Drive (please let me know if this is correct!)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True) # Forcing remount just in case\n",
        "\n",
        "# Constants - Question 1\n",
        "q1_n_epochs = 3\n",
        "q1_show_summary = False\n",
        "\n",
        "# Constants - Question 2\n",
        "q2_learning_rate = 0.001\n",
        "q2_batch_size = 64\n",
        "q2_show_summary = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtgCJz1aqVSg"
      },
      "source": [
        "# **Question 1<br>**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Load data (and specify dimensional arguments for reference later)*"
      ],
      "metadata": {
        "id": "MjljKIf_rhgW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PMLBqwi0qVSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a15c5f-8bff-4059-f287-ea210246a135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Expected shape for MNIST dataset: (28, 28, 1)\n",
            "Actual shape for MNIST dataset..: (28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "in_shape = (28, 28, 1) # 28x28x1 image\n",
        "\n",
        "x_train, x_test = np.expand_dims(x_train, -1), np.expand_dims(x_test, -1)\n",
        "y_train, y_test = keras.utils.to_categorical(y_train, 10), keras.utils.to_categorical(y_test, 10)\n",
        "print(f\"Expected shape for MNIST dataset: {in_shape}\\nActual shape for MNIST dataset..: {x_train.shape[1:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Increasing-size model***"
      ],
      "metadata": {
        "id": "eujbRFzErppm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B43PJRgqqVSi"
      },
      "outputs": [],
      "source": [
        "conv_model_increasing = keras.Sequential([keras.layers.Input(shape=in_shape),\n",
        "                                          keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=48, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                                          keras.layers.Conv2D(filters=80, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=96, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=112, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                                          keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=144, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=170, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                                          keras.layers.Flatten(),\n",
        "                                          keras.layers.Dense(10, activation=\"softmax\")])\n",
        "if q1_show_summary: conv_model_increasing.summary()\n",
        "\n",
        "# Compile & train\n",
        "conv_model_increasing.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "conv_model_increasing.fit(x_train, y_train, epochs=q1_n_epochs)\n",
        "\n",
        "# Evaluate\n",
        "print(\"=\"*10 + \"\\nEvaluating...\\n\" + \"=\"*10)\n",
        "eval_score = conv_model_increasing.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"    Loss: {eval_score[0]}\\nAccuracy: {eval_score[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Decreasing-size model***"
      ],
      "metadata": {
        "id": "ejozLGCQrz4F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrCbxvmQqVSj"
      },
      "outputs": [],
      "source": [
        "conv_model_decreasing = keras.Sequential([keras.layers.Input(shape=in_shape),\n",
        "                                          keras.layers.Conv2D(filters=170, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=144, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                                          keras.layers.Conv2D(filters=112, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=96, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=80, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                                          keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=48, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n",
        "                                          keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                                          keras.layers.Flatten(),\n",
        "                                          keras.layers.Dense(10, activation=\"softmax\")])\n",
        "if q1_show_summary: conv_model_decreasing.summary()\n",
        "\n",
        "# Compile & train\n",
        "conv_model_decreasing.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "conv_model_decreasing.fit(x_train, y_train, epochs=q1_n_epochs)\n",
        "\n",
        "# Evaluate\n",
        "print(\"=\"*10 + \"\\nEvaluating...\\n\" + \"=\"*10)\n",
        "eval_score = conv_model_decreasing.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"    Loss: {eval_score[0]}\\nAccuracy: {eval_score[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hourglass-shaped model (is this a VAE?)***"
      ],
      "metadata": {
        "id": "ZnCYlD0wr5sg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-PHD2DkqVSk"
      },
      "outputs": [],
      "source": [
        "vae_model = keras.Sequential([keras.layers.Input(shape=in_shape),\n",
        "                              keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.Conv2D(filters=96, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                              keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                              keras.layers.Conv2D(filters=96, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n",
        "                              keras.layers.MaxPooling2D(pool_size=2, strides=1, padding=\"same\"),\n",
        "                              keras.layers.Flatten(),\n",
        "                              keras.layers.Dense(10, activation=\"softmax\")])\n",
        "if q1_show_summary: vae_model.summary()\n",
        "\n",
        "# Compile & train\n",
        "vae_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "vae_model.fit(x_train, y_train, epochs=q1_n_epochs)\n",
        "\n",
        "# Evaluate\n",
        "print(\"=\"*10 + \"\\nEvaluating...\\n\" + \"=\"*10)\n",
        "eval_score = vae_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"    Loss: {eval_score[0]}\\nAccuracy: {eval_score[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2**"
      ],
      "metadata": {
        "id": "iPmFXFqiu6FV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9653latXqVSn"
      },
      "source": [
        "*Load data (and specify dimensional arguments for reference later)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DsWi3B7lqVSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5796e50d-6b9f-4313-efa7-9c7f2a5b3387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n",
            "Expected shape for CIFAR10 dataset: (32, 32, 3)\n",
            "Actual shape for CIFAR10 dataset..: (32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "in_shape = (32, 32, 3) # 32x32 image, with depth of 3 (RGB color channels)\n",
        "\n",
        "y_train, y_test = keras.utils.to_categorical(y_train, 10), keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Expected shape for CIFAR10 dataset: {in_shape}\\nActual shape for CIFAR10 dataset..: {x_train.shape[1:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRu6sl8yqVSn"
      },
      "source": [
        "***Build model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "khw_wnN-qVSo"
      },
      "outputs": [],
      "source": [
        "lenet_model = keras.Sequential([keras.layers.Input(shape=in_shape),\n",
        "                                keras.layers.Conv2D(filters=6, kernel_size=5, strides=1),\n",
        "                                keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                                keras.layers.Conv2D(filters=16, kernel_size=5, strides=1),\n",
        "                                keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                                keras.layers.Conv2D(filters=120, kernel_size=5, strides=1),\n",
        "                                keras.layers.Flatten(),\n",
        "                                keras.layers.Dense(84),\n",
        "                                keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "if q2_show_summary: lenet_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train & evaluate model***"
      ],
      "metadata": {
        "id": "QLboT2O4w4dQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dvHYV8pqVSo"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "cust_adam = keras.optimizers.Adam(learning_rate=q2_learning_rate)\n",
        "lenet_model.compile(loss=\"categorical_crossentropy\", optimizer=cust_adam, metrics=[\"accuracy\"])\n",
        "lenet_model.fit(x_train, y_train, epochs=25, batch_size=q2_batch_size)\n",
        "\n",
        "# Evaluate model (for funsies, mostly since we already have test splits)\n",
        "print(\"=\"*10 + \"\\nEvaluating...\\n\" + \"=\"*10)\n",
        "eval_score = lenet_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"    Loss: {eval_score[0]}\\nAccuracy: {eval_score[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build feedforward model***"
      ],
      "metadata": {
        "id": "iMIwYRGtlmA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build FF model\n",
        "ff_model = keras.Sequential([keras.layers.Input(shape=in_shape),\n",
        "                             keras.layers.Flatten(),\n",
        "                             keras.layers.Dense(6),\n",
        "                             keras.layers.Dense(16),\n",
        "                             keras.layers.Dense(120),\n",
        "                             keras.layers.Dense(84),\n",
        "                             keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "if q2_show_summary: ff_model.summary()"
      ],
      "metadata": {
        "id": "FRYz40AljIlm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train & evaluate FF model***"
      ],
      "metadata": {
        "id": "Zm_0cJxHlsA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "cust_adam = keras.optimizers.Adam(learning_rate=q2_learning_rate)\n",
        "ff_model.compile(loss=\"categorical_crossentropy\", optimizer=cust_adam, metrics=[\"accuracy\"])\n",
        "ff_model.fit(x_train, y_train, epochs=25, batch_size=q2_batch_size)\n",
        "\n",
        "# Evaluate model (for funsies, mostly since we already have test splits)\n",
        "print(\"=\"*10 + \"\\nEvaluating...\\n\" + \"=\"*10)\n",
        "eval_score = ff_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"    Loss: {eval_score[0]}\\nAccuracy: {eval_score[1]}\")"
      ],
      "metadata": {
        "id": "5j333wtnjp7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Question 3**"
      ],
      "metadata": {
        "id": "as1YE9iEmHHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used NumPy here for easier submatrix slicing. It would have been easy to do this in pure native Python as well, though. I hope using NumPy doesn't violate the \"your own code\" part of the assignment description.\n",
        "\n",
        "I also tried to not use hard-coded slicing intervals, so that you can change the input and/or filter to prove that the code works for several inputs (to show comprehension). The only thing hard-coded is the pooling kernel size (2x2), and the stride for both the conv and pool layers."
      ],
      "metadata": {
        "id": "QFqiy4WLsOSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and F\n",
        "x = np.matrix([[7, 5, 0, 0, 3, 2],\n",
        "               [6, 4, 5, 1, 4, 8],\n",
        "               [9, 0, 2, 2, 5, 4],\n",
        "               [6, 3, 4, 7, 9, 8],\n",
        "               [5, 7, 5, 6, 9, 0],\n",
        "               [7, 9, 0, 8, 2, 3]])\n",
        "\n",
        "f = np.matrix([[1, 0, -1],\n",
        "               [2, 0, -2],\n",
        "               [1, 0, -1]])\n",
        "\n",
        "# Tell dimensions\n",
        "print(f\"Input dimension: {x.shape}\\nKernel dimensions: {f.shape}\")\n",
        "print(\"-\"*10)\n",
        "\n",
        "# Tell output activation map\n",
        "activation_map = np.zeros_like(f)\n",
        "\n",
        "for row in range(len(x)-len(f)):\n",
        "  for col in range(x.shape[1]-f.shape[1]):\n",
        "    activation_map[row,col] = np.sum(np.multiply(x[row:row+f.shape[0], col:col+f.shape[1]], f))\n",
        "\n",
        "print(f\"Output activation map:\\n{activation_map}\")\n",
        "print(\"-\"*10)\n",
        "\n",
        "# Tell max-pooling result\n",
        "pool_output = np.zeros((2, 2))\n",
        "\n",
        "for row in range(1+activation_map.shape[0]-pool_output.shape[0]):\n",
        "  for col in range(1+activation_map.shape[1]-pool_output.shape[1]):\n",
        "    pool_output[row,col] = np.max(activation_map[row:row+pool_output.shape[0], col:col+pool_output.shape[1]])\n",
        "\n",
        "print(f\"Max pooling output:\\n{pool_output}\")"
      ],
      "metadata": {
        "id": "r5Lp69ZVmVaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1 Report**\n",
        "---\n",
        "Probably to no one's surprise, the best-performing option was with the default parameters. I might point out that increasing the batch size did allow the network to converge faster, at the cost of a very small amount of accuracy (less than 2% at worst). After 3 epochs, the accuracy still increased, but by marginal amounts (less than 1% per epoch). I tried other optimizer functions, but Adam worked best; even in the [Keras documentation](https://keras.io/api/optimizers/adam/), Adam is described as being a generally good all-around optimizer, and is often used.\n",
        "\n",
        "The accuracy for the training set is reported in evaluation for each network."
      ],
      "metadata": {
        "id": "x4LbfjVYBXBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2 Report**\n",
        "---\n",
        "1. Having a learning rate below the default (0.001) still allowed for successful training, but it did cause the rate at which the network converged to be lower. Having a learning rate above the default would essentially always result in no convergence, or a lower accuracy.\n",
        "2. Having a batch size slightly above the default (128 max), would still allow for decent training results, while dramatically increasing speed per epoch. Lowering it would increase (marginally) the accuracy, but would dramatically increase training time.\n",
        "3. The performance was pretty poor overall on this network, with a best result of ~70% accuracy using default hyperparameters (batchsize=32, learning rate=0.001).\n",
        "4. The performance was mostly comparable, with a slight decrease in accuracy overall (approx 65%), though much better throughput. One thing I noticed is that the FF model could greatly benefit from a dropout layer, as its accuracy dropped during each epoch.\n"
      ],
      "metadata": {
        "id": "-wOm60b9_sd5"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}